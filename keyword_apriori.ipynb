{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "from itertools import chain, combinations\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stemming.porter2 import stem\n",
    "import wikipedia\n",
    "from gensim import corpora, models\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsets(arr):\n",
    "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
    "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def returnfrequentitemsets(itemSet, transactionList, minSupport, freqSet):\n",
    "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
    "        of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
    "        _itemSet = set()\n",
    "        localSet = defaultdict(int)\n",
    "\n",
    "        for item in itemSet:\n",
    "            for transaction in transactionList:\n",
    "                if item.issubset(transaction):\n",
    "                    freqSet[item] += 1\n",
    "                    localSet[item] += 1\n",
    "\n",
    "        for item, count in localSet.items():\n",
    "            support = float(count)/len(transactionList)\n",
    "\n",
    "            if support >= minSupport:\n",
    "                _itemSet.add(item)\n",
    "\n",
    "        return _itemSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def joinSet(itemSet, length):\n",
    "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
    "        return set([i.union(j) for i in itemSet for j in itemSet if len(i.union(j)) == length])\n",
    "\n",
    "def getSupport(transactionList,freqSet,item):\n",
    "    return float(freqSet[item])/len(transactionList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runApriori(data_iter, minSupport):\n",
    "\n",
    "    transactionList=list()\n",
    "    itemSet=set()\n",
    "    for record in data_iter:\n",
    "        transaction=frozenset(record)\n",
    "        transactionList.append(transaction)\n",
    "        for item in transaction:\n",
    "            itemSet.add(frozenset([item]))             # generate 1-itemset\n",
    "    #print type(itemSet)\n",
    "    freqSet = defaultdict(int)\n",
    "    largeSet = dict()\n",
    "    #assocRules = dict()\n",
    "    # Dictionary which stores Association Rules\n",
    "\n",
    "    oneCSet = returnfrequentitemsets(itemSet,transactionList,minSupport,freqSet)  # obtaining frequent 1-itemset          \n",
    "\n",
    "    currentLSet = oneCSet\n",
    "    k = 2\n",
    "    while(currentLSet != set([])):\n",
    "        \"\"\" while loop generates 2 or more itemsets that are frequent and the result is stored in \n",
    "            largeSet dictionary\"\"\"      \n",
    "        largeSet[k-1] = currentLSet\n",
    "        currentLSet = joinSet(currentLSet, k)        # joining frequent itemset to obtain k length itemset\n",
    "        currentCSet = returnfrequentitemsets(currentLSet,transactionList,minSupport,freqSet)    # from the set obtained by joining select the itemsets that satisfy minsupport\n",
    "        currentLSet = currentCSet\n",
    "        k = k + 1\n",
    "    RetrieveItems = []\n",
    "    \"\"\" RetrieveItems store itemset which are frequent with their support\"\"\"\n",
    "    for key, value in largeSet.items():\n",
    "        for item in value:\n",
    "            RetrieveItems.append((tuple(item),getSupport(transactionList,freqSet,item))) \n",
    "        #RetrieveItems.extend([(tuple(item), getSupport(transactionList,freqSet,item))\n",
    "                        #for item in value])\n",
    "    return RetrieveItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printfreqitemsets(items,maxlength,writer): \n",
    "    count=countfreqitemsets(items,maxlength)\n",
    "    #count=list(count)\n",
    "    #writer.write('%d' %count)\n",
    "    for item, support in sorted(items, key=lambda (item, support): support):\n",
    "    #print type(item)\n",
    "        if len(item)==maxlength:    # printing frequent itemsets with max length\n",
    "            itemlist=list(item)\n",
    "            #print type(itemlist)\n",
    "            #freq_item=str(\",\".join([str(s) for s in itemlist]))\n",
    "            #freq_itemList=freq_item.split(' ')\n",
    "            #print freq_itemList\n",
    "            writer.writerows([itemlist])\n",
    "            #print \"%s\" % str(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(sentence).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((u'obama',), 0.5600858369098712)]\n"
     ]
    }
   ],
   "source": [
    "# tokenize to sentences based on the notations followed in english literature\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#import wikipedia page for \n",
    "content = wikipedia.page('barack').content\n",
    "sentences = []\n",
    "sentences = text_to_sentences(content, tokenizer)\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# stem token\n",
    "sentences = [[stem(word) for word in sentence] for sentence in sentences]\n",
    "\n",
    "minsupport = 0.5\n",
    "items = runApriori(sentences, minsupport)\n",
    "print items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
