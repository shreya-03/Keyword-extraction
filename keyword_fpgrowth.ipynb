{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stemming.porter2 import stem\n",
    "import wikipedia\n",
    "from gensim import corpora, models\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FPNode(object):\n",
    "\n",
    "    def __init__(self, value, count, parent):\n",
    "        self.value = value\n",
    "        self.count = count\n",
    "        self.parent = parent\n",
    "        self.link = None\n",
    "        self.children = []\n",
    "\n",
    "    def has_child(self, value):\n",
    "        for node in self.children:\n",
    "            if node.value == value:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_child(self, value):\n",
    "        for node in self.children:\n",
    "            if node.value == value:\n",
    "                return node\n",
    "        return None\n",
    "\n",
    "    def add_child(self, value):\n",
    "        child = FPNode(value, 1, self)\n",
    "        self.children.append(child)\n",
    "        return child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FPTree(object):\n",
    "\n",
    "    def __init__(self, transactions, threshold, root_value, root_count):\n",
    "        self.frequent = self.find_frequent_items(transactions, threshold)\n",
    "        #print self.frequent\n",
    "        self.headers = self.build_header_table(self.frequent)\n",
    "        self.root = self.build_fptree(\n",
    "            transactions, root_value,\n",
    "            root_count, self.frequent, self.headers)\n",
    "\n",
    "    def find_frequent_items(self,transactions, threshold):\n",
    "        items = defaultdict(lambda: 0) # mapping from items to their supports\n",
    "        for transaction in transactions:\n",
    "            for item in transaction:\n",
    "                items[item] += 1\n",
    "        items = dict((item, support) for item, support in items.iteritems()\n",
    "        if support >= threshold)\n",
    "        return items\n",
    "\n",
    "    def build_header_table(self,frequent):\n",
    "        headers = {}\n",
    "        for key in frequent.keys():\n",
    "            headers[key] = None\n",
    "        return headers\n",
    "\n",
    "    def build_fptree(self, transactions, root_value,root_count, frequent, headers):\n",
    "        root = FPNode(root_value, root_count, None)\n",
    "\n",
    "        for transaction in transactions:\n",
    "            sorted_items = [x for x in transaction if x in frequent]\n",
    "            sorted_items.sort(key=lambda x: frequent[x], reverse=True)\n",
    "            if len(sorted_items) > 0:\n",
    "                self.insert_tree(sorted_items, root, headers)\n",
    "        return root\n",
    "\n",
    "    def insert_tree(self, items, node, headers):\n",
    "        first = items[0]\n",
    "        child = node.get_child(first)\n",
    "        if child is not None:\n",
    "            child.count += 1\n",
    "        else:\n",
    "            # Add new child.\n",
    "            child = node.add_child(first)\n",
    "\n",
    "            # Link it to header structure.\n",
    "            if headers[first] is None:\n",
    "                headers[first] = child\n",
    "            else:\n",
    "                current = headers[first]\n",
    "                while current.link is not None:\n",
    "                    current = current.link\n",
    "                current.link = child\n",
    "\n",
    "        # Call function recursively.\n",
    "        remaining_items = items[1:]\n",
    "        if len(remaining_items) > 0:\n",
    "            self.insert_tree(remaining_items, child, headers)\n",
    "\n",
    "    def tree_has_single_path(self, node):\n",
    "        num_children = len(node.children)\n",
    "        if num_children > 1:\n",
    "            return False\n",
    "        elif num_children == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return True and self.tree_has_single_path(node.children[0])\n",
    "\n",
    "    def mine_patterns(self, threshold):\n",
    "        \"\"\"\n",
    "        Mine the constructed FP tree for frequent patterns.\n",
    "        \"\"\"\n",
    "        if self.tree_has_single_path(self.root):\n",
    "            return self.generate_pattern_list()\n",
    "        else:\n",
    "            return self.zip_patterns(self.mine_sub_trees(threshold))\n",
    "\n",
    "    def zip_patterns(self, patterns):\n",
    "        \"\"\"\n",
    "        Append suffix to patterns in dictionary if\n",
    "        we are in a conditional FP tree.\n",
    "        \"\"\"\n",
    "        suffix = self.root.value\n",
    "\n",
    "        if suffix is not None:\n",
    "            # We are in a conditional tree.\n",
    "            new_patterns = {}\n",
    "            for key in patterns.keys():\n",
    "                new_patterns[tuple(sorted(list(key) + [suffix]))] = patterns[key]\n",
    "\n",
    "            return new_patterns\n",
    "\n",
    "        return patterns\n",
    "\n",
    "    def generate_pattern_list(self):\n",
    "        \"\"\"\n",
    "        Generate a list of patterns with support counts.\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        items = self.frequent.keys()\n",
    "\n",
    "        # If we are in a conditional tree,\n",
    "        # the suffix is a pattern on its own.\n",
    "        if self.root.value is None:\n",
    "            suffix_value = []\n",
    "        else:\n",
    "            suffix_value = [self.root.value]\n",
    "            patterns[tuple(suffix_value)] = self.root.count\n",
    "\n",
    "        for i in range(1, len(items) + 1):\n",
    "            for subset in itertools.combinations(items, i):\n",
    "                pattern = tuple(sorted(list(subset) + suffix_value))\n",
    "                patterns[pattern] = \\\n",
    "                    min([self.frequent[x] for x in subset])\n",
    "\n",
    "        return patterns\n",
    "\n",
    "    def mine_sub_trees(self, threshold):\n",
    "        \"\"\"\n",
    "        Generate subtrees and mine them for patterns.\n",
    "        \"\"\"\n",
    "        patterns = {}\n",
    "        mining_order = sorted(self.frequent.keys(),\n",
    "                                key=lambda x: self.frequent[x])\n",
    "\n",
    "        # Get items in tree in reverse order of occurrences.\n",
    "        for item in mining_order:\n",
    "            suffixes = []\n",
    "            conditional_tree_input = []\n",
    "            node = self.headers[item]\n",
    "\n",
    "            # Follow node links to get a list of\n",
    "            # all occurrences of a certain item.\n",
    "            while node is not None:\n",
    "                suffixes.append(node)\n",
    "                node = node.link\n",
    "\n",
    "            # For each occurrence of the item, \n",
    "            # trace the path back to the root node.\n",
    "            for suffix in suffixes:\n",
    "                frequency = suffix.count\n",
    "                path = []\n",
    "                parent = suffix.parent\n",
    "\n",
    "                while parent.parent is not None:\n",
    "                    path.append(parent.value)\n",
    "                    parent = parent.parent\n",
    "\n",
    "                for i in range(frequency):\n",
    "                    conditional_tree_input.append(path)\n",
    "\n",
    "            # Now we have the input for a subtree,\n",
    "            # so construct it and grab the patterns.\n",
    "            subtree = FPTree(conditional_tree_input, threshold,\n",
    "                            item, self.frequent[item])\n",
    "            subtree_patterns = subtree.mine_patterns(threshold)\n",
    "\n",
    "            # Insert subtree patterns into main patterns dictionary.\n",
    "            for pattern in subtree_patterns.keys():\n",
    "                if pattern in patterns:\n",
    "                    patterns[pattern] += subtree_patterns[pattern]\n",
    "                else:\n",
    "                    patterns[pattern] = subtree_patterns[pattern]\n",
    "\n",
    "        return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_frequent_patterns(transactions, support_threshold):\n",
    "    tree = FPTree(transactions, support_threshold, None, None)\n",
    "    return tree.mine_patterns(support_threshold)\n",
    "\n",
    "def subsets(arr):\n",
    "    return itertools.chain(*[itertools.combinations(arr, i + 1) for i, a in enumerate(arr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse sentences to words and remove stopwords from sentence\n",
    "def sentence_to_wordlist( sentence, remove_stopwords=True ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    text = BeautifulSoup(sentence).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to parse text to sentences using tokenizer mentioned above\n",
    "def text_to_sentences(text,tokenizer,remove_stopwords=True):\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( sentence_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(u'obama',): 300}\n"
     ]
    }
   ],
   "source": [
    "# tokenize to sentences based on the notations followed in english literature\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#import wikipedia page for \n",
    "content = wikipedia.page('barack').content\n",
    "sentences = []\n",
    "sentences = text_to_sentences(content, tokenizer)\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "# stem token\n",
    "sentences = [[stem(word) for word in sentence] for sentence in sentences]\n",
    "\n",
    "minsupport=0.5*len(sentences)\n",
    "items = find_frequent_patterns(sentences, minsupport)\n",
    "print items"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
